"""
This module creates a testbench for the learning algorithm which illustrates
its progress graphically. The problem is a tolopogy where the objective is to
get to the lowest altitude possible.

Note: TestBench uses (row, column) (y, x) cordinate convention for consistency
with array indexing.
"""

import numpy as np
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
try:
    from qlearner import QLearner
    from flearner import FLearner
    from slearner import SLearner
    from linsim import FlagGenerator
    from tb_utils import abs_cartesian
    from tb_utils import fault_algorithm
    from tb_utils import create_sim_env
except ImportError:
    from . import QLearner
    from . import FLearner
    from . import SLearner
    from .linsim import FlagGenerator
    from .tb_utils import abs_cartesian
    from .tb_utils import fault_algorithm
    from .tb_utils import create_sim_env



class TestBench:
    """
    The testbench class simulates the learning procedure implemented by the
    Qlearner class graphically for illustration. It generates a geographical
    system - a square grid where each point has a height. The goal states are
    coordinates with lower heights. Possible actions/transitions are movements
    to adjacent points. There are two state variables: x and y coords. Each
    instance automatically generates transition and reward matrices that can
    be used by a QLearner instance.

    Args:
        size (int): The size of each side of the topology (size * size points).
        seed (int): The seed for the random number generator.
        method (str): Method for generating topology. Default='fault'.
        goals (int/list): Number of goal states. Default = size. If list, then
            (y, x) coordinates of goal states in topology.
        wrap (bool): Whether or not the topology wraps at edges. Default=False.
        learner (class): QLearner or a subclass. Instantiated by set_up_learner()
            using keyword arguments passed at instantiation and the relevant
            [r|t]matrices generated by the TestBench. If None, the user is left
            to assign to self.learner.
        **kwargs: A sequence of keyword arguments to instantiate the qlearner
            if one is not provided. Any keywords except tmatrix, rmatrix, and
            goals which are provided by the TestBench.

    Instance Attributes:
        random (np.random.RandomState): A random number generator local to each
            instance of TestBench.
        topology (2D ndarray): A square array of heights defining the system.
            Heights are stored as [y, x] coordinates in line with the array
            indexing convention.
        size (int): Size of topology (length of side).
        num_states (int): Number of possible states/positions in the system.
        actions (2D ndarray): An array where each row defines an action by a
            change in topology coordinates. For e.g. [1,0] is move-up.
        path (list): Sequence of coordinates (y, x) tuples taken on topology to
            reach goal state. Populated by episode() function.
        tmatrix (2D ndarray): The transition matrix.
        rmatrix (2D ndarray): The reward matrix.
        goals (list): List of goal state numbers (coords encoded into int).
        num_goals (int): Number of goal states.
        qlearner (QLearner): QLearner instance or a subclass. The learn()
            function must be called before visualizing the learned policy function.
        fig (plt.figure): A matplotlib figure instance storing all plots.
        topo_ax (Axes3D): An axis object storing the 3D plots.
        topo_surface (Poly3DCollection): Contains plotted surface.
        path_line (list): Stores a list of lists of all Line3D segments plotted
            on the figure. Populated after show_topology(). path_line[0] will
            be a list of Line3D instances representing the first line and so on.
    """

    plot_num = -1

    def __init__(self, size=10, seed=0, method='fault', goals=None, wrap=False,
                 learner=QLearner, **kwargs):
        self.random = np.random.RandomState(seed)
        self.seed = seed
        # qlearning params
        self.topology = np.zeros((size, size))
        self.size = size
        self.num_states = size * size
        self.actions = np.array([[0, -1], [0, 1], [-1, 0], [1, 0]])
        self.path = []
        self.tmatrix = np.array([])
        self.rmatrix = np.array([])

        if isinstance(goals, (int, float)):
            self.goals = []
            self.num_goals = abs(int(goals))
        elif isinstance(goals, (list, tuple, np.ndarray)):
            goals = [self.coord2state(g) for g in goals]
            self.goals = [g for g in goals if g >= 0 and g < self.num_states]
            self.num_goals = len(self.goals)
        else:
            self.goals = []
            self.num_goals = self.size

        self.learner = None

        # plotting variables
        self.__class__.plot_num += 1
        self.fig_num = self.__class__.plot_num
        self.fig = None

        self.topology = self.create_topology(method)
        self.tmatrix = self.create_tmatrix(wrap=wrap)
        if len(self.goals) == 0:
            self.goals = self.create_goals(self.topology, self.num_goals)
        self.rmatrix = self.create_rmatrix(self.goals, self.topology, self.tmatrix)
        self.set_up_learner(learner, **kwargs)


    def set_up_learner(self, learner, **kwargs):
        """
        Attaches the appropriate learner to instance for testing.
        """
        if learner == FLearner:
            sflags = FlagGenerator(self.size, self.size)
            aflags = FlagGenerator(2, 2)
            self.learner = FLearner(rmatrix=self.rmatrix, goal=self.goals,
                                    stateconverter=sflags, actionconverter=aflags,
                                    tmatrix=self.tmatrix, seed=self.seed, **kwargs)
        elif learner == QLearner:
            self.learner = QLearner(rmatrix=self.rmatrix, goal=self.goals,
                                    tmatrix=self.tmatrix, seed=self.seed, **kwargs)

        elif learner == SLearner:
            sflags = FlagGenerator(self.size, self.size)
            aflags = FlagGenerator(2, 2)
            sim = create_sim_env(self.size, self.random)
            def reward(svec, avec, nstate):
                action = aflags.encode(avec)
                state = sflags.encode((round(svec[0]), round(svec[1])))
                return self.rmatrix[state, action]
            def goal(svec):
                return self.coord2state((round(svec[0]), round(svec[1]))) in self.goals
            self.learner = SLearner(reward=reward, simulator=sim, goal=goal,
                                    stateconverter=sflags, actionconverter=aflags,
                                    seed=self.seed, **kwargs)
        elif learner is None:
            self.learner = None
        else:
            raise TypeError('Class: ' + learner.__name__ + ' is not supported.\
                             Assign to .learner manually')


    def episode(self, start=None, interactive=True, limit=-1):
        """
        Run a single episode from the provided qlearner. The episode starts at
        coordinates 'start' and ends when it reaches a goal state. Calls the
        self.learner.recommend() function to get sequence of actions to take
        based on the learned Q-Matrix.

        Args:
            start (list/tuple/ndarray): y and x coordinates to start from. If
                None, generates random coordinates. Of the form (y, x).
            interactive (bool): If true, shows the plot. Else returns a list
                of coordinates (y,x) traversed on path.
            limit (int): Maximum number of steps in episode before quitting.
                Defaults to self.size*self.size.

        Return:
            A list of coordinates stored in self.path that were traversed to
            reach the goal state. Only when interactive=False. The list is a
            reference to self.path.
        """
        if start is None:
            start = (self.random.randint(self.size), self.random.randint(self.size))
        self.path = [tuple(start)]
        limit = self.size**2 if limit <= 0 else limit
        iteration = 0
        try:                # for integer state representation
            current = self.coord2state(start)
            while not self.learner.goal(current) and iteration < limit:
                iteration += 1
                action = self.learner.recommend(current)
                current = self.learner.next_state(current, action)
                self.path.append(self.state2coord(current))
        except TypeError:   # for vector state representation
            current = start
            while not self.learner.goal(current) and iteration < limit:
                iteration += 1
                action = self.learner.recommend(current)
                current = self.learner.next_state(current, action)
                self.path.append(current)
        if interactive:
            self.show_topology(QPath=self.path)
        else:
            return self.path


    def shortest_path(self, point, metric=abs_cartesian):
        """
        Returns the shortest path between the point and any of the goal states
        where the distance between two adjacent states/points is determined by
        the metric. Uses Dijkstra's algorithm.

        Args:
            point (tuple/list/ndarray): (y, x) coordinates of point.
            metric (func): A function that calculates the measure of distance
                between two points on the topology. Signature is:
                    func(topology, source, target)
                Where topology is self.topology, source is the source point,
                and target is the point to which the distance is measured.
                All points are (y, x) coordinates. Returns a positive float.

        Returns:
            A list containing the optimal path from the point to one of the
            goal states. The list contains points on the topology (y, x)
            traversed.
        """
        # Set up initial distances, visited status, and target states.
        # All states intially loop back to themselves.
        distances = np.array([(i, np.inf) for i in range(self.num_states)])
        distances[self.coord2state(point)][1] = 0
        predecessors = np.arange(self.num_states)
        unvisited = np.ones(self.num_states, dtype=bool)
        goals_left = set(self.goals)

        # Explore closest state from source as long as there are targets/states left
        while len(goals_left):
            closest = int(distances[unvisited][np.argmin(distances[unvisited], axis=0)[1]][0])
            unvisited[closest] = False
            if closest in goals_left:
                goals_left.remove(closest)

            neighbours = [n for n in self.tmatrix[closest, :] if unvisited[n]]
            for n in neighbours:
                distance = metric(self.topology, self.state2coord(closest),\
                                self.state2coord(n))
                if distance < distances[n, 1]:
                    distances[n, 1] = distances[closest, 1] + distance
                    predecessors[n] = closest

        # Find closest goal state and trace path back to source
        goal_distances = distances[self.goals, 1]
        closest_goal = self.goals[np.argmin(goal_distances)]
        path = [self.state2coord(closest_goal)]
        state = closest_goal
        prev = predecessors[state]
        while prev != state:
            path.insert(0, self.state2coord(prev))
            state = prev
            prev = predecessors[state]
        # Check if last element in path is goal state
        if path[-1] != self.state2coord(closest_goal):
            raise ValueError('Shortest path could not be found.')
        elif path[0] != tuple(point):
            raise ValueError('Could not trace goal to starting point.')
        return path


    def show_topology(self, showfield=False, showlegend=False, **paths):
        """
        Draws a surface plot of the topology, marks goal states, and any episode
        up to its current progress.

        Args:
            showfield (bool): Whether to show a field plot of optimal actions
                on each state.
            **paths: A sequence of keyword arguments describing paths to plot
                on the topology. They should be of the form:
                <PATH_NAME>=[LIST OF (y, x) COORDINATE PAIRS]
        """
        # Set up figure and axes
        self.fig = plt.figure(self.fig_num)
        if showfield:
            topo_ax = self.fig.add_subplot(121, aspect='equal', projection='3d')
            field_ax = self.fig.add_subplot(122, aspect='equal')
            field_ax.invert_yaxis()
        else:
            topo_ax = self.fig.add_subplot(111, projection='3d')
        path_line = []
        topo_ax.invert_yaxis()
        # Plot 3d topology surface
        x, y = np.meshgrid(np.linspace(0, self.size-1, self.size),
                           np.linspace(0, self.size-1, self.size))
        z = self.topology.reshape(x.shape)
        topo_ax.plot_surface(x, y, z, cmap='gist_earth')
        # Plot optimal action field
        if showfield:
            vals, inds = list(zip(*[self.learner.value(s) for s in range(self.num_states)]))
            inds = np.array(inds, dtype=int)
            vals = np.array(vals, dtype=float)
            vals = vals - min(vals) + 0.1       # adding small value in case all vals are same and reduce to 0
            vals = vals / max(vals)
            # action_y multiplied by negative val since y-axes are inverted (bug)
            try:
                inds = [self.learner.actionconverter.encode(i) for i in inds]
            except (TypeError, AttributeError):
                pass
            action_y = self.actions[inds][:, 0] * -vals
            action_x = self.actions[inds][:, 1] * vals
            field_ax.quiver(np.ravel(x), np.ravel(y),
                            action_x, action_y, np.ravel(z), cmap='gist_earth',
                            pivot='mid')
        # Plot goal states
        gc = [self.state2coord(i) for i in self.goals]  # goal coords
        gz = [self.topology[g[0], g[1]] for g in gc]
        gx = [g[1] for g in gc]
        gy = [g[0] for g in gc]
        topo_ax.scatter(gx, gy, gz)
        # Plot path
        for path, coords in sorted(paths.items(), key=lambda x: x[0]):
            px = [p[1] for p in coords]
            py = [p[0] for p in coords]
            pz = [self.topology[int(round(p[0])), int(round(p[1]))] for p in coords]
            path_line.append(topo_ax.plot(px, py, pz, label=path))
        # Set labels
        topo_ax.set_xlabel('X')
        topo_ax.set_ylabel('Y')
        topo_ax.set_zlabel('Altitude')
        topo_ax.set_title('Topology')
        if showfield:
            field_ax.set_xlabel('X')
            field_ax.set_ylabel('Y')
            field_ax.set_title('Optimal Action Field')
        if len(paths) > 0 and showlegend:
            topo_ax.legend()
        # Display figure
        plt.show()
        self.fig.clear()
        plt.close(self.fig_num)


    def create_topology(self, method='fault', *args, **kwargs):
        """
        Creates a square height map based on one of several terrain generation
        algorithms. The topology is stored in self.topology (2D ndarray), where
        self.topology[y, x] = height at coordinate (x, y).

        Args:
            method (str/func): The algorithm to use. Default='fault'. OR it can
                also be a function object. The function must return a ndarray
                consistent with the topology size given to TestBench at
                instantiation. Signature like:
                    function(self, *args, **kwargs)
            *args: Positional arguments passed on to method if it is a function.
            **kwargs: Keyword arguments passed on to method if it is a function.
        
        Returns:
            A size x size array representing a height map.
        """
        if callable(method):
            return method(*args, **kwargs)
        elif method == 'fault':
            return fault_algorithm(int(self.random.rand() * 200),\
                            (self.size, self.size), self.random)


    def create_tmatrix(self, wrap=False):
        """
        Creates a transition matrix based on the action vectors (self.actions).
        
        Args:
            wrap (bool): Makes actions on edges go to the other side.
        
        Returns:
            A states x actions ndarray. Where [i, j] is the next state index
            for taking action j from state i.
        """
        tmatrix = np.zeros((self.num_states, len(self.actions)), dtype=int)
        for i in range(self.num_states):
            coords = self.state2coord(i)
            # updating r and t matrices
            for j, action in enumerate(self.actions):
                next_coord = coords + action
                if wrap:
                    next_coord[next_coord < 0] += self.size
                    next_coord[next_coord >= self.size] -= self.size
                else:
                    next_coord[next_coord < 0] = 0
                    next_coord[next_coord >= self.size] = self.size - 1

                tmatrix[i, j] = self.coord2state(next_coord)
        return tmatrix
    

    def create_goals(self, topology, num_goals):
        """
        Returns a list of the num_goals lowest elevation states indices.

        Args:
            topology (ndarray): A size x size array where each element is the
                height at that coordinate [y, x].
            num_goals (int): Number of goal states to create.
        """
        return np.argsort(np.ravel(topology))[:num_goals]
    

    def create_rmatrix(self, goals, topology, tmatrix):
        """
        Given goals, topology, and tmatrix, calculates the reward for taking
        action j from state i.

        Args:
            goals (list): State indices of goal states (see create_goals()).
            topology (ndarray): A size x size height map (see create_topology().
            tmatrix (ndarray): A states x actions array (see create_tmatrix()).
        
        Returns:
            A states x actions array where element [i, j] is reward for taking
            action j from state i.
        """
        reward_lim = np.amax(topology) - np.amin(topology)
        rmatrix = np.zeros((self.num_states, len(self.actions)))
        for i in range(self.num_states):
            coords = self.state2coord(i)
            for j, action in enumerate(self.actions):
                next_coord = self.state2coord(tmatrix[i, j])
                if tmatrix[i, j] in goals:
                    rmatrix[i, j] = 1
                else:
                    rmatrix[i, j] = (topology[coords[0], coords[1]] \
                                     - topology[next_coord[0], next_coord[1]]) \
                                    / reward_lim \
                                    - (1/self.size)
        return rmatrix


    def generate_trg(self, wrap=False):
        """
        NOTE: DEPRACATED
        Calculates goal states and creates transition & reward matrices. Where
        tmatrix[state, action] points to index of next state. And
        rmatrix[state, action] is the reward for taking that action. State is
        he encoded state number from coords2state. The transitions roll over:
        i.e going right from the right-most coordinate takes to the left-most
        coordinate.

        Args:
            wrap (bool): Whether or not the topology wraps around boundaries.
                Default = False.

        Returns:
            A tuple of (transition matrix, reward matrix, and list of goal states)
        """
        reward_lim = np.amax(self.topology) - np.amin(self.topology)
        tmatrix = np.zeros((self.num_states, len(self.actions)), dtype=int)
        rmatrix = np.zeros((self.num_states, len(self.actions)))
        # updating goal states
        goals = np.argsort(np.ravel(self.topology))[:self.num_goals] \
                if len(self.goals) == 0 else self.goals

        for i in range(self.num_states):
            coords = self.state2coord(i)
            # updating r and t matrices
            for j, action in enumerate(self.actions):
                next_coord = coords + action
                if wrap:
                    next_coord[next_coord < 0] += self.size
                    next_coord[next_coord >= self.size] -= self.size
                else:
                    next_coord[next_coord < 0] = 0
                    next_coord[next_coord >= self.size] = self.size - 1

                tmatrix[i, j] = self.coord2state(next_coord)
                # Punish actions looping back i.e at edges when no wrap
                # if np.array_equal(next_coord, coords):
                #     rmatrix[i, j] = -reward_lim
                # Reward actions leading to final goal
                if tmatrix[i, j] in goals:
                    rmatrix[i, j] = self.size*reward_lim
                else:
                    rmatrix[i, j] = (self.topology[coords[0], coords[1]] \
                                    - self.topology[next_coord[0], next_coord[1]]) \
                                    / reward_lim - 1
        return (tmatrix, rmatrix, goals)


    def coord2state(self, coord):
        """
        Encodes coordinates into a state number that can be used as an index.
        Essentially converts Base_size coordinates into Base_10.

        Args:
            coord (tuple/list/ndarray): 2 coordinates [row, column]

        Returns:
            An integer representing coordinates.
        """
        return int(self.size * coord[0] + coord[1])


    def state2coord(self, state):
        """
        Converts a state number into two coordinates. Essentially converts
        Base_10 state into Base_size coordinates.

        Args:
            state (int): An integer representing a state.

        Returns:
            A 2 element tuple of [row, column] coordinates.
        """
        return (int(state/self.size), state % self.size)
